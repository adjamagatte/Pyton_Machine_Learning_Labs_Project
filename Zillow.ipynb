{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Zillow.ipynb","provenance":[],"authorship_tag":"ABX9TyOPcYcfX/FC/DLm8JaP0XYP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# GOAL: Build a mini-app to accelerate how to find the best apartment"],"metadata":{"id":"lZ749Mg8C87T"}},{"cell_type":"markdown","source":["This Python labs sessions has one objective, build a data science solution which  answers this question: where are the best apartment/houses based on some characteristics. Among the activities : \n","* will learn/consolidate how to do web scraping **(it assumes you worked on the prposed tutorials at the previous sessions)**\n","* will use pandas **(to organize the data)**\n","* will create features to give actionable insights **(distance to beach, criminality...)**\n","* will learn how to visualize **(histograms, box-plots)**\n","* will learn how to create a *mini-app*"],"metadata":{"id":"pQpWPhu7xPPQ"}},{"cell_type":"markdown","source":["* **NO** dangling code & NO functions without comments\n","\n","* Good practices when writing variable names\n"," - **LINTING:** https://realpython.com/python-code-quality/\n"," - **VERSION CONTROL:** CODE must be sync'ed with Github "],"metadata":{"id":"RjGjVLy1DSpY"}},{"cell_type":"markdown","source":["If you have followed the previous tutorials you should be able to understand the code below:"],"metadata":{"id":"N8brWOvlKe4o"}},{"cell_type":"code","source":["!pip install selenium\n","!apt-get update \n","!apt install chromium-chromedriver"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MdSaAswhFRxp","executionInfo":{"status":"ok","timestamp":1653832288028,"user_tz":420,"elapsed":44016,"user":{"displayName":"Assan Sanogo","userId":"09058940399104567928"}},"outputId":"8f338a06-5630-42b4-ccd1-5678ca480931"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting selenium\n","  Downloading selenium-4.2.0-py3-none-any.whl (983 kB)\n","\u001b[K     |████████████████████████████████| 983 kB 2.8 MB/s \n","\u001b[?25hCollecting trio~=0.17\n","  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n","\u001b[K     |████████████████████████████████| 359 kB 31.5 MB/s \n","\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n","  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 15.3 MB/s \n","\u001b[?25hCollecting trio-websocket~=0.9\n","  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n","Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n","Collecting sniffio\n","  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n","Collecting async-generator>=1.9\n","  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Collecting outcome\n","  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n","Collecting wsproto>=0.14\n","  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.5.18.1)\n","Collecting pyOpenSSL>=0.14\n","  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 2.3 MB/s \n","\u001b[?25hCollecting cryptography>=1.3.4\n","  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 41.7 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n","Collecting h11<1,>=0.9.0\n","  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.2.0)\n","Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed async-generator-1.10 cryptography-37.0.2 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.2.0 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n","Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [787 kB]\n","Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:14 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.8 kB]\n","Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [954 kB]\n","Get:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n","Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,792 kB]\n","Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,507 kB]\n","Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,990 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [988 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,223 kB]\n","Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,021 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [29.8 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,281 kB]\n","Get:25 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n","Get:26 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.3 kB]\n","Fetched 16.0 MB in 4s (3,613 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n","Suggested packages:\n","  webaccounts-chromium-extension unity-chromium-extension\n","The following NEW packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-chromedriver\n","  chromium-codecs-ffmpeg-extra\n","0 upgraded, 4 newly installed, 0 to remove and 63 not upgraded.\n","Need to get 89.8 MB of archives.\n","After this operation, 302 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 101.0.4951.64-0ubuntu0.18.04.1 [1,142 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 101.0.4951.64-0ubuntu0.18.04.1 [78.5 MB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 101.0.4951.64-0ubuntu0.18.04.1 [4,980 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 101.0.4951.64-0ubuntu0.18.04.1 [5,153 kB]\n","Fetched 89.8 MB in 5s (19.9 MB/s)\n","Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n","(Reading database ... 155629 files and directories currently installed.)\n","Preparing to unpack .../chromium-codecs-ffmpeg-extra_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser.\n","Preparing to unpack .../chromium-browser_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser-l10n.\n","Preparing to unpack .../chromium-browser-l10n_101.0.4951.64-0ubuntu0.18.04.1_all.deb ...\n","Unpacking chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-chromedriver.\n","Preparing to unpack .../chromium-chromedriver_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n","Setting up chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for mime-support (3.60ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n"]}]},{"cell_type":"code","source":["from selenium import webdriver\n","chrome_options = webdriver.ChromeOptions()\n","chrome_options.add_argument('--headless')\n","chrome_options.add_argument('--no-sandbox')\n","chrome_options.add_argument('--disable-dev-shm-usage')\n","driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iiCjbU6IFVWH","executionInfo":{"status":"ok","timestamp":1653833296435,"user_tz":420,"elapsed":864,"user":{"displayName":"Assan Sanogo","userId":"09058940399104567928"}},"outputId":"da56e123-62d8-47ef-a72d-daafbf0f66e5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: use options instead of chrome_options\n","  \n"]}]},{"cell_type":"code","source":["wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n","res = wd.get(\"https://www.zillow.com/homes/for_rent/?searchQueryState=%7B%22mapBounds%22%3A%7B%22west%22%3A-117.28203672784423%2C%22east%22%3A-117.19406027215575%2C%22south%22%3A32.7746319816297%2C%22north%22%3A32.82405263546272%7D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22price%22%3A%7B%22min%22%3A0%2C%22max%22%3A832043%7D%2C%22mp%22%3A%7B%22min%22%3A0%2C%22max%22%3A3500%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%2C%22fr%22%3A%7B%22value%22%3Atrue%7D%2C%22fsba%22%3A%7B%22value%22%3Afalse%7D%2C%22fsbo%22%3A%7B%22value%22%3Afalse%7D%2C%22nc%22%3A%7B%22value%22%3Afalse%7D%2C%22cmsn%22%3A%7B%22value%22%3Afalse%7D%2C%22auc%22%3A%7B%22value%22%3Afalse%7D%2C%22fore%22%3A%7B%22value%22%3Afalse%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A14%7D\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"957rW46LFps8","executionInfo":{"status":"ok","timestamp":1653833298201,"user_tz":420,"elapsed":1769,"user":{"displayName":"Assan Sanogo","userId":"09058940399104567928"}},"outputId":"3b41e399-66de-432a-f427-4c90fb1c57fc"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: use options instead of chrome_options\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}]},{"cell_type":"code","source":["driver.page_source"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"U0mPZi94GJYU","executionInfo":{"status":"ok","timestamp":1653833310249,"user_tz":420,"elapsed":135,"user":{"displayName":"Assan Sanogo","userId":"09058940399104567928"}},"outputId":"1052dcb4-7deb-4079-9abe-22cefe664cd4"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<html><head></head><body></body></html>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["Is selenium working ? Please explain"],"metadata":{"id":"go1d_T5UKmV0"}},{"cell_type":"markdown","source":["please setup a fake email account:\n","https://temp-mail.org/fr/\n","and **sign in Zillow**"],"metadata":{"id":"vTlHu01xKtuF"}},{"cell_type":"markdown","source":["**WARNING:**\n","When you perform webscraping there are aspects you must have in mind:\n","* do not send too many requests from your own computer (add a **time.sleep**)\n","* we can use **AWS lambda functions** to do crawling as the IP changes at each launch\n","* we can also use **IP rotation**\n","* headless Selenium is easily detected by websites like Facebook (as potential bots)\n","* Methods against us (cat and mouse):\n","👉 https://www.scraping-bot.io/anti-scraping-methods/\n","👉 https://stackoverflow.com/questions/54984185/webpage-is-detecting-selenium-webdriver-with-chromedriver-as-a-bot"],"metadata":{"id":"vHzOoRfkLCiL"}},{"cell_type":"markdown","source":["# Step 1: Analysis of Zillow"],"metadata":{"id":"RP2ySlqSy1A8"}},{"cell_type":"markdown","source":["You must ask yourself:\n","\n","* What is an API ?\n","- please check : data.gouv\n","*   \"Is there any API ?\" (check the availability)\n","*   \"is there an easy structure (html/css) to help scraping?\"\n","\n","the main page has a searchbox\n","* how do you send commands to a searchbox ?\n","* how do you make sure that you keep track of the window / (in theory vs in this case ?)\n","* what. about \"seloger.fr\" ?\n","* is it the only way to access the data ?\n"],"metadata":{"id":"ONHNh0Z0zCPV"}},{"cell_type":"markdown","source":["what information can you extract ?\n","  zillow.com/homes/for_rent/?searchQueryState=%7B\"pagination\"%3A%7B%7D%2C\"mapBounds\"%3A%7B\"west\"%3A-119.57072509765625%2C\"east\"%3A-114.70927490234375%2C\"south\"%3A31.210330412499946%2C\"north\"%3A34.372794188002125%7D%2C\"isMapVisible\"%3Atrue%2C\"filterState\"%3A%7B\"fsba\"%3A%7B\"value\"%3Afalse%7D%2C\"fsbo\"%3A%7B\"value\"%3Afalse%7D%2C\"nc\"%3A%7B\"value\"%3Afalse%7D%2C\"fore\"%3A%7B\"value\"%3Afalse%7D%2C\"cmsn\"%3A%7B\"value\"%3Afalse%7D%2C\"auc\"%3A%7B\"value\"%3Afalse%7D%2C\"fr\"%3A%7B\"value\"%3Atrue%7D%2C\"ah\"%3A%7B\"value\"%3Atrue%7D%2C\"mp\"%3A%7B\"max\"%3A3500%7D%2C\"price\"%3A%7B\"max\"%3A832043%7D%7D%2C\"isListVisible\"%3Atrue%2C\"mapZoom\"%3A8%7D\n","  "],"metadata":{"id":"E4tY8V7w0hmK"}},{"cell_type":"markdown","source":["### You can try to click on the website to understand:\n","* what is the **pagination** ?\n","* if you sort data by **increasing** or **descending order** - what do you see in the url ? what are the categories of appartements Zillow has created ?\n"],"metadata":{"id":"E9FWhOj61CBx"}},{"cell_type":"markdown","source":["## TL:DR - Scraping is hard.\n","Selenium is not always the solution and  you might have to be inventive. <br>\n","👉\n","Because of this lab we will use:\n","* **webscraper.io** \n","* **this tutorial**:\n","https://medium.com/fortune-for-future/how-to-scrape-zillow-data-for-free-without-writing-any-code-be2ac698e604#:~:text=Click%20%E2%80%9CSitemap%20zillow%E2%80%9D%20in%20the,load%20it%20into%20your%20spreadsheet.\n","\n"],"metadata":{"id":"18GlQ8SHxsQK"}},{"cell_type":"markdown","source":[" Here is the json document u will use to extract **all listings** for 1 page and generalize for **n pages**"],"metadata":{"id":"ihrycrhAyeAp"}},{"cell_type":"markdown","source":["{\n","    \"_id\": \"forum-zillow-2021-e\",\n","    \"startUrl\": [\n","        \"https://www.zillow.com/pacific-beach-san-diego-ca/rentals/?searchQueryState=%7B%22pagination%22%3A%7B%7D%2C%22mapBounds%22%3A%7B%22west%22%3A-117.55014065786041%2C%22east%22%3A-117.12854031606354%2C%22south%22%3A32.77951971643358%2C%22north%22%3A32.869533572035685%7D%2C%22mapZoom%22%3A12%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A117156%2C%22regionType%22%3A8%7D%5D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22fore%22%3A%7B%22value%22%3Afalse%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%2C%22auc%22%3A%7B%22value%22%3Afalse%7D%2C%22nc%22%3A%7B%22value%22%3Afalse%7D%2C%22fr%22%3A%7B%22value%22%3Atrue%7D%2C%22fsbo%22%3A%7B%22value%22%3Afalse%7D%2C%22cmsn%22%3A%7B%22value%22%3Afalse%7D%2C%22fsba%22%3A%7B%22value%22%3Afalse%7D%7D%2C%22isListVisible%22%3Atrue%7D\"\n","    ],\n","    \"selectors\": [\n","\n","        {\n","            \"id\": \"Separate scroller\",\n","            \"type\": \"SelectorElementScroll\",\n","            \"parentSelectors\": [\n","                \"_root\"\n","            ],\n","            \"selector\": \"div#grid-search-results > ul > li:nth-of-type(2n+3)\",\n","            \"multiple\": true,\n","            \"delay\": \"2500\",\n","            \"scrollElementSelector\": \"div.search-page-list-container\"\n","        },\n","        {\n","            \"id\": \"Item wrappers\",\n","            \"type\": \"SelectorElement\",\n","            \"parentSelectors\": [\n","                \"_root\"\n","            ],\n","            \"selector\": \"div#grid-search-results > ul > li\",\n","            \"multiple\": true\n","        },\n","        {\n","            \"id\": \"Price\",\n","            \"type\": \"SelectorText\",\n","            \"parentSelectors\": [\n","                \"Item wrappers\"\n","            ],\n","            \"selector\": \"div.list-card-price\",\n","            \"multiple\": false,\n","            \"regex\": \"\"\n","        },\n","        {\n","            \"id\": \"Details\",\n","            \"type\": \"SelectorText\",\n","            \"parentSelectors\": [\n","                \"Item wrappers\"\n","            ],\n","            \"selector\": \"ul.list-card-details\",\n","            \"multiple\": false,\n","            \"regex\": \"\"\n","        },\n","        {\n","            \"id\": \"Link\",\n","            \"type\": \"SelectorElementAttribute\",\n","            \"parentSelectors\": [\n","                \"Item wrappers\"\n","            ],\n","            \"selector\": \"a.list-card-link\",\n","            \"multiple\": false,\n","            \"extractAttribute\": \"href\"\n","        }\n","    ]\n","}\n","   "],"metadata":{"id":"Z_crI227xf3z"}},{"cell_type":"markdown","source":["Now we have all listings (page webs), we still need to get the information, for that we are going to instruct the computer to scroll down and capture the text until we are able to extract  all the information we need.\n","\n","For time constraints (making the scraper getting elements we are going to use a *quick fix* \n","\n","for that end we will need: **autogui** + **Pytesseract** + **Opencv**"],"metadata":{"id":"6yLtbmCxyoBv"}},{"cell_type":"code","source":["import pyautogui\n","import numpy as np\n","pyautogui.moveTo(100, 100, duration=0.25)"],"metadata":{"id":"Lh134i__2jl9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# simple example with opencv\n","import matplotlib.pyplot as plt\n","img = plt.imread(\"/vignettes_zillow.png\") # open an image\n","img_uint8 = (img*255).astype(np.uint8) # convert the image to uint8\n","plt.imshow(img_uint8) # display the image\n","\n","img_uint8[270:310, 350:450,:3].shape # \"zoom\"on the borders\n","plt.imshow(img_uint8[270:310, 350:450,1])\n","img_uint8[270:310, 350:450,:3][20:30,20:30,1]"],"metadata":{"id":"DyjCV4YV2n4d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 2:  Data collection"],"metadata":{"id":"PAq6mE2ny3KY"}},{"cell_type":"markdown","source":["### What  data are you going to collect ?\n","* The MOST important feature is the proximity to the ocean (what feature are you going to consider?)\n","* Should we limit the search with parameters of get all data and filter later on\n","  - what are the consequences ?\n","* On Zillow websites the headlines are **not acurate**  (=apartments listings not available in reality)\n","* In the US, individuals pick their housing based on the **quality of the nearby schools **. where do you get this information ?\n","* What about getting details about **walk-friendliness** (what feature are you going to extract?)\n","* What about getting details about the **zone accesibility by car** (where should I look ?)\n","* Are you going to collect unstructured data ? **(if yes how?)** \n"],"metadata":{"id":"mzh4uYFH32f5"}},{"cell_type":"markdown","source":["# Step 3: Features creation"],"metadata":{"id":"Zu1subjr5y9v"}},{"cell_type":"code","source":["# HOW-TO-PANDAS:\n","# https://www.youtube.com/watch?v=CmorAWRsCAw&list=PLeo1K3hjS3uuASpe-1LjfG5f14Bnozjwy "],"metadata":{"id":"biIJ3wJK-srt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["👉   we need to build analytics:\n","* premium appartments **(what makes an appartment premium ?)**\n","- can you derive a data-based feature?\n"],"metadata":{"id":"-PR_sEBt565T"}},{"cell_type":"markdown","source":["👉 we **cannot have lists** inside a cell of a pandas dataframe if we want to use them in a ML model\n","* there is quite some free-text how can be convert or extract elements of interest ?\n","- can you derive a data-based features?"],"metadata":{"id":"ELKds5Om-Ynd"}},{"cell_type":"code","source":["# MUST WATCH: https://www.youtube.com/playlist?list=PLeo1K3hjS3uuvuAXhYjV2lMEShq2UYSwX\n","# https://spacy.io/usage/linguistic-features\n","# https://regex101.com/\n"],"metadata":{"id":"utosKhjl8IsB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 4: Build a ML solution"],"metadata":{"id":"noES8ER68Kpa"}},{"cell_type":"code","source":["# HOW-TO-ML: \n","# scikit-learn - ABC of ML: https://www.youtube.com/watch?v=pqNCD_5r0IU&t=617s\n","# how-to-compare models: https://www.youtube.com/watch?v=0pP4EwWJgIU\n","# spacy - nlp: https://www.youtube.com/watch?v=7PD48PFL9VQ"],"metadata":{"id":"rGHAFX2F66z0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In all ML problems, you will have:\n","1. training set\n","2. validation set (optional)\n","3. test set\n","\n","\n","- on the \"training set\" you **train your model**\n","- on the \"validation set\" you **optimize your model**\n","- on the \"test set\" **you evaluate the peformance**\n","\n"],"metadata":{"id":"6hdj4ZDq9UwM"}},{"cell_type":"markdown","source":["### a.How are you going to separate your dataset ?\n","* is the test set distribution similar to **the train set ?**"],"metadata":{"id":"8UPDG7XSA_dR"}},{"cell_type":"markdown","source":["### b. How is your dataset ? balanced not balanced ?\n","* is it a problem ?\n","* what can you do ?"],"metadata":{"id":"ljXIbXl_BU3d"}},{"cell_type":"markdown","source":["### c.What type of ML model are you going to build ?\n","* regression\n","* classification\n","* cluster"],"metadata":{"id":"YqKLqMMYBLWk"}},{"cell_type":"markdown","source":["### d. Build your model"],"metadata":{"id":"wvJ9Y4DdCvqJ"}},{"cell_type":"code","source":["# HOW-TO-ML: \n","# ADVANCED:  https://www.youtube.com/watch?v=p_7hJvl7P2A\n","# ADVANCED (pipelines I): https://www.youtube.com/watch?v=xIqX1dqcNbY\n","# ADVANCED (pipelines II): https://www.youtube.com/watch?v=jzKSAeJpC6s\n"],"metadata":{"id":"9FNev27ax5J9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 5. Visualize"],"metadata":{"id":"-gGqhbky3RY6"}},{"cell_type":"code","source":["# https://positionstack.com/ (longitude-latitude data)\n","# plotly - https://plotly.com/python/scattermapbox/\n","# https://streamlit.io/  (build app)"],"metadata":{"id":"5_ZdsUBa3Qxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"t95Zh-364LtA"},"execution_count":null,"outputs":[]}]}